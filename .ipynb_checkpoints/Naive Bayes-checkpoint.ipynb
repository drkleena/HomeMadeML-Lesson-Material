{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes: A Good Starting Point\n",
    "\n",
    "<p>The naive bayes learner is a simple yet effective machine learning model that is a great starting point to learn about the task of classification. What we're trying to do is predict the most likely class for a given instance. In other words, for a given instance, what is the probability it belongs to a certain class. Our prediction is then based on the class with the highest probability. </p>\n",
    "\n",
    "<p>What we're trying to do:</p>\n",
    "\n",
    "$\\widehat{C} = argmax_{c_j\\in C} P(c_j|T)$\n",
    "\n",
    "<p>Through bayes rule we can transform the equation</p>\n",
    "\n",
    "$\\widehat{C} = argmax_{c_j\\in C} P(T|c_j)*P(c_j)$\n",
    "\n",
    "<p>Since T is an instance where $T = <x_1, x_2, x_3, ... , x_n>$</p>\n",
    "\n",
    "$\\widehat{C} = argmax_{c_j\\in C} P(x_1, x_2, x_3, ... , x_n|c_j)*P(c_j)$\n",
    "\n",
    "<p>The 'naive' part: Conditional Independance Assumption</p>\n",
    "\n",
    "<p>We can't directly calculate $P(x_1, x_2, x_3, ... , x_n|c_j)$, so we do something 'dumb', we assume that all our instance attributes are <b>conditionally independant</b> on the class. Rarely does this seem to exist in most datasets but this assumption does not impede on the model's ability to predict classes</p>\n",
    "\n",
    "<p>This conditional independance assumption translates into the following:</p>\n",
    "\n",
    "$P(x_1, x_2, x_3, ... , x_n|c_j) ≈ P(x_1|c_j)P(x_2|c_j)P(x_3|c_j)...P(x_n|c_j)$\n",
    "\n",
    "$≈\\prod_{i} P(x_i|c_j)$\n",
    "\n",
    "In order to classify an instance we simply do the following:\n",
    "1. Find the prior probability of the class\n",
    "2. Multiply each attribute's corresponding posterior probability by one another\n",
    "3. Take 1 and multiply that by 2\n",
    "4. \n",
    "\n",
    "#### Example with a toy dataset:\n",
    "\n",
    "|Headache|Sore  |Temperature|Cough|Diagnosis|\n",
    "|--------|------|-----------|-----|---------|\n",
    "|severe  |mild  |high       |yes |flu     |\n",
    "|no      |severe|normal     |yes |cold     |\n",
    "|mild    |mild  |normal     |yes |flu     |\n",
    "|mild    |no    |normal     |no |cold     |\n",
    "|severe  |severe|normal     |yes |flu     |\n",
    "\n",
    "\n",
    "#### Calculate our priors and posteriors:\n",
    "|Flu                                                                        |Cold                                                                       |\n",
    "|:--------------------------------------------------------------------------|---------------------------------------------------------------------------|\n",
    "|$\\text{P}(\\text{Flu})=\\frac{3}{5}$                                         |$\\text{P}(\\text{Cold})=\\frac{2}{5}$    \n",
    "|$\\text{P}(\\text{Headache} = \\textit{severe } | \\text{ Flu})=\\frac{2}{3}$   |$\\text{P}(\\text{Headache} = \\textit{severe } | \\text{ Cold})=\\frac{0}{2}$|\n",
    "|$\\text{P}(\\text{Headache} = \\textit{mild } | \\text{ Flu})=\\frac{1}{3}$     |$\\text{P}(\\text{Headache} = \\textit{mild } | \\text{ Cold})=\\frac{1}{2}$|\n",
    "|$\\text{P}(\\text{Headache} = \\textit{no } | \\text{ Flu})=\\frac{0}{3}$       |$\\text{P}(\\text{Headache} = \\textit{no } | \\text{ Cold})=\\frac{1}{2}$|\n",
    "|$\\text{P}(\\text{Sore} = \\textit{severe } | \\text{ Flu})=\\frac{1}{3}$       |$\\text{P}(\\text{Sore} = \\textit{severe } | \\text{ Cold})=\\frac{1}{2}$|\n",
    "|$\\text{P}(\\text{Sore} = \\textit{mild } | \\text{ Flu})=\\frac{2}{3}$         |$\\text{P}(\\text{Sore} = \\textit{mild } | \\text{ Cold})=\\frac{0}{2}$|\n",
    "|$\\text{P}(\\text{Sore} = \\textit{no } | \\text{ Flu})=\\frac{0}{3}$           |$\\text{P}(\\text{Sore} = \\textit{no } | \\text{ Cold})=\\frac{1}{2}$|\n",
    "|$\\text{P}(\\text{Temperature} = \\textit{high } | \\text{ Flu})=\\frac{1}{3}$  |$\\text{P}(\\text{Temperature} = \\textit{high } | \\text{ Cold})=\\frac{0}{2}$|\n",
    "|$\\text{P}(\\text{Temperature} = \\textit{normal } | \\text{ Flu})=\\frac{2}{3}$|$\\text{P}(\\text{Temperature} = \\textit{normal } | \\text{ Cold})=\\frac{2}{2}$|\n",
    "|$\\text{P}(\\text{Cough} = \\textit{yes } | \\text{ Flu})=\\frac{3}{3}$         |$\\text{P}(\\text{Cough} = \\textit{yes } | \\text{ Cold})=\\frac{1}{2}$|\n",
    "|$\\text{P}(\\text{Cough} = \\textit{no } | \\text{ Flu})=\\frac{0}{3}$          |$\\text{P}(\\text{Cough} = \\textit{no } | \\text{ Cold})=\\frac{1}{2}$|\n",
    "\n",
    "### Classification Example\n",
    "\n",
    "1. Someone comes to the clinic with a mild headache, severe soreness, normal temperature and no cough. Are they more likely to have a cold, or the flu?\n",
    "\n",
    "    \n",
    "    For each possible class, we calculate the probability of the instance being part of that class, and then take the class with the highest probability as our prediction\n",
    "\n",
    "    Probability of patient having cold:\n",
    "\n",
    "$$\\text{P(Cold) x P(Headache = mild|Cold) x P(Sore = severe|Cold) x P(Temperature = no|Cold) x P(Cough = no|Cold)}$$\n",
    "$$= (\\frac{2}{5})(\\frac{1}{2})(\\frac{1}{2})(\\frac{2}{2})(\\frac{1}{2})$$\n",
    "$$= 0.05$$\n",
    "\n",
    "    Probability of patient having flu:\n",
    "$$\\text{P(Flu) x P(Headache = mild|Flue) x P(Sore = severe|Flue) x P(Temperature = no|Flue) x P(Cough = no|Flu)}$$\n",
    "$$= (\\frac{3}{5})(\\frac{1}{3})(\\frac{1}{3})(\\frac{2}{3})(\\frac{0}{3})$$\n",
    "$$= 0$$\n",
    "\n",
    "The NB learner would choose Cold as the classification of this instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation: The Fun Part\n",
    "\n",
    "### Priors\n",
    "Getting our priors for each class in the dataset is easy: simple counting is only needed. We can do this through the following,\n",
    "\n",
    "```python\n",
    "# assuming we have a pandas dataframe containing our entire dataset with labels\n",
    "def get_priors(df):\n",
    "        class_freq = dd(int)\n",
    "        for item in df['class']:\n",
    "            class_freq[item] += 1 \n",
    "        sum_ = sum(class_freq.values())\n",
    "        for item in class_freq.keys():\n",
    "            class_freq[item] = class_freq[item]/sum_    \n",
    "        return class_freq\n",
    "```\n",
    "What our priors look like for a 4 class dataset:\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "car_dataset = preprocess_supervised(\"car\")\n",
    "nb = NaiveBayes(car_dataset)\n",
    "\n",
    "nb.priors = defaultdict(int,\n",
    "            {'acc': 0.2222222222222222,\n",
    "             'good': 0.03993055555555555,\n",
    "             'unacc': 0.7002314814814815,\n",
    "             'vgood': 0.03761574074074074})\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posteriors\n",
    "\n",
    "Now getting our posteriors is a bit tricky due to the triple dictionary structure of our implementation\n",
    "\n",
    "```python\n",
    "def get_posteriors(self):\n",
    "        df = self.df\n",
    "        posterior_dict = {}\n",
    "        class_freq = dd(int)\n",
    "        for item in df['class']:\n",
    "            class_freq[item] += 1\n",
    "        for attribute in list(df)[0:-1]:\n",
    "            # for each attribute value we need a dictionary of attribute-value: class counts\n",
    "            posterior_dict[attribute] = {}\n",
    "        # this allows ease of access for the different classes in the dataset\n",
    "        dictinct_class_values = df[\"class\"].unique()\n",
    "\n",
    "        for x in posterior_dict:\n",
    "            for values in dictinct_class_values:\n",
    "                distinct_cell_values = df[x].unique()\n",
    "                posterior_dict[x].update({values:{}})\n",
    "                for things in distinct_cell_values:\n",
    "                    posterior_dict[x][values].update({things:0})\n",
    "\n",
    "        for row in range(len(df.index)):\n",
    "            for attribute in list(df)[0:-1]:\n",
    "                # do not contribute missing data to probability counts\n",
    "                \n",
    "                if(posterior_dict[attribute][df[\"class\"][row]][df[attribute][row]]!=\"?\"):\n",
    "                \n",
    "                    posterior_dict[attribute][df[\"class\"][row]][df[attribute][row]]+=1\n",
    "                    \n",
    "        for key, value in posterior_dict.items():\n",
    "            for key1, value1 in value.items():\n",
    "                for key2, value2 in value1.items():\n",
    "                    posterior_dict[key][key1][key2] = value2/class_freq[key1]\n",
    "        return posterior_dict\n",
    "```\n",
    "What our posterior dict looks like for the same 'car' dataset\n",
    "\n",
    "```python\n",
    "nb.posteriors =\n",
    "{0: {'acc': {'high': 0.28125,\n",
    "   'low': 0.23177083333333334,\n",
    "   'med': 0.2994791666666667,\n",
    "   'vhigh': 0.1875},\n",
    "  'good': {'high': 0.0,\n",
    "   'low': 0.6666666666666666,\n",
    "   'med': 0.3333333333333333,\n",
    "   'vhigh': 0.0},\n",
    "  'unacc': {'high': 0.26776859504132233,\n",
    "   'low': 0.21322314049586777,\n",
    "   'med': 0.22148760330578512,\n",
    "   'vhigh': 0.2975206611570248},\n",
    "  'vgood': {'high': 0.0, 'low': 0.6, 'med': 0.4, 'vhigh': 0.0}},\n",
    " 1: {'acc': {'high': 0.2734375,\n",
    "   'low': 0.23958333333333334,\n",
    "   'med': 0.2994791666666667,\n",
    "   'vhigh': 0.1875},\n",
    "  'good': {'high': 0.0,\n",
    "   'low': 0.6666666666666666,\n",
    "   'med': 0.3333333333333333,\n",
    "   'vhigh': 0.0},\n",
    "  'unacc': {'high': 0.25950413223140495,\n",
    "   'low': 0.22148760330578512,\n",
    "   'med': 0.22148760330578512,\n",
    "   'vhigh': 0.2975206611570248},\n",
    "  'vgood': {'high': 0.2, 'low': 0.4, 'med': 0.4, 'vhigh': 0.0}},\n",
    " 2: {'acc': {'2': 0.2109375, '3': 0.2578125, '4': 0.265625, '5more': 0.265625},\n",
    "  'good': {'2': 0.21739130434782608,\n",
    "   '3': 0.2608695652173913,\n",
    "   '4': 0.2608695652173913,\n",
    "   '5more': 0.2608695652173913},\n",
    "  'unacc': {'2': 0.2694214876033058,\n",
    "   '3': 0.24793388429752067,\n",
    "   '4': 0.2413223140495868,\n",
    "   '5more': 0.2413223140495868},\n",
    "  'vgood': {'2': 0.15384615384615385,\n",
    "   '3': 0.23076923076923078,\n",
    "   '4': 0.3076923076923077,\n",
    "   '5more': 0.3076923076923077}},\n",
    " 3: {'acc': {'2': 0.0, '4': 0.515625, 'more': 0.484375},\n",
    "  'good': {'2': 0.0, '4': 0.5217391304347826, 'more': 0.4782608695652174},\n",
    "  'unacc': {'2': 0.47603305785123967,\n",
    "   '4': 0.2578512396694215,\n",
    "   'more': 0.26611570247933886},\n",
    "  'vgood': {'2': 0.0, '4': 0.46153846153846156, 'more': 0.5384615384615384}},\n",
    " 4: {'acc': {'big': 0.375, 'med': 0.3515625, 'small': 0.2734375},\n",
    "  'good': {'big': 0.34782608695652173,\n",
    "   'med': 0.34782608695652173,\n",
    "   'small': 0.30434782608695654},\n",
    "  'unacc': {'big': 0.30413223140495865,\n",
    "   'med': 0.3239669421487603,\n",
    "   'small': 0.371900826446281},\n",
    "  'vgood': {'big': 0.6153846153846154,\n",
    "   'med': 0.38461538461538464,\n",
    "   'small': 0.0}},\n",
    " 5: {'acc': {'high': 0.53125, 'low': 0.0, 'med': 0.46875},\n",
    "  'good': {'high': 0.43478260869565216, 'low': 0.0, 'med': 0.5652173913043478},\n",
    "  'unacc': {'high': 0.22892561983471074,\n",
    "   'low': 0.47603305785123967,\n",
    "   'med': 0.2950413223140496},\n",
    "  'vgood': {'high': 1.0, 'low': 0.0, 'med': 0.0}}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_supervised(file, normal = True):\n",
    "    if(normal):\n",
    "        df = pd.read_csv(\"./2018S1-proj1_data/\"+file+\".csv\",header=None)\n",
    "        unnamed = df.columns[len(df.columns)-1]\n",
    "        df.rename(columns={unnamed:'class'},inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from NaiveBayes import *\n",
    "car_dataset = preprocess_supervised(\"car\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_train_test_split(df, split):\n",
    "\n",
    "    train_percent, test_percent = split[0], split[1]\n",
    "\n",
    "    # randomise our dataset and reset the indexes\n",
    "    df_shuffled = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    #find the indexes we want to slice\n",
    "    train_portion_index = int(len(df_shuffled.index)//((1/train_percent)))\n",
    "\n",
    "    test_portion_index = train_portion_index+1\n",
    "\n",
    "    #slice the instances according to the split ratio\n",
    "    train_portion = df_shuffled[:train_portion_index].reset_index(drop=True)\n",
    "\n",
    "    test_portion = df_shuffled[test_portion_index-1:].reset_index(drop=True)\n",
    "\n",
    "    return train_portion, test_portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, test = get_train_test_split(car_dataset, (0.1,0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb = NaiveBayes(car_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87274041937816338"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(nb.predict(test)[\"class\"] == nb.predict(test)[\"predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1383"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
