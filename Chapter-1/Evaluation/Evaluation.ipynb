{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "“Everybody wants to be a bodybuilder, but nobody wants to lift no heavy-ass weights.”\n",
    "― Ronnie Coleman\n",
    "\n",
    "Often we hear alot of about neural networks, deep learning, convolutional networks and deep cross vectored reinforced bayes systems - well the last one was a joke but nevertheless it sounds pretty 'juicy'. Alot of the time people want to dive head first into these sexy machine learning techniques but neglect learning about the grunt work that comes with them. Evaluation is a crucial if not paramount step in machine learning, that along with preprocessing the data, takes a developer the bulk of the effort trying to get right. Building a model is all well and good but if you cannot asses its performance in a meaningful and demonstrable way then it might as well be left behind. Throughout this series, we will take ourselfs through a journey of assesing a model under various fascets of evaluation and critically analyse these different methods\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which instances to test? Which instances to train? Both train and test?\n",
    "\n",
    "Lets say we have a model and mountains of data to train it on. One's first thought would be to simply train the model on the whole dataset and then test the model on the same dataset as a whole. Although this sounds great on paper it is deeply flawed and is the mortal sin in machine learning. If there's one thing to be taken away it is DO NOT TEST ON TRAINING DATA! - ever. Imagine taking a subject where you learn the answers to a test prior to taking the test, would your knowledge be fairly assesed? - no definetely not. Therefore we must think of our task of training and evaluation as involving two seperate datasets (sampled from the same population - more on this later). This leads us to our first evaluation strategy - Holdout.\n",
    "\n",
    "## Split Strategy 1: Holdout\n",
    "\n",
    "What we do here is randomly sample x% of the dataset with no replacement and use this as train and the remaining instances as test. This leads to the testing dataset to be 'held out'. \n",
    "\n",
    "#### Lets look at quick implementation of this:\n",
    "\n",
    "What we want to do is take a random selection of x% for training and 1-x% for testing:\n",
    "\n",
    "```python\n",
    "def get_train_test_split(df, split):\n",
    "    \n",
    "    train_percent, test_percent = split[0], split[1]\n",
    "\n",
    "    # randomise our dataset and reset the indexes\n",
    "    df_shuffled = df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    #find the indexes we want to slice\n",
    "    train_portion_index = int(len(df_shuffled.index)/train_percent)\n",
    "    \n",
    "    test_portion_index = train_portion_index+1\n",
    "    \n",
    "    #slice the instances according to the split ratio\n",
    "    train_portion = df_shuffled[:train_portion_index].reset_index(drop=True)\n",
    "    \n",
    "    test_portion = df_shuffled[test_portion_index-1:].reset_index(drop=True)\n",
    "    \n",
    "    return train_portion, test_portion\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "#### We're done now right?\n",
    "no, simple holdout still gives us the burden of choosing the 'right' split ratio. A large amount of training data will allow the model to see more instances and thus become better trained. But with this comes a small amount of test data that doesn't really allow us to test the generalisability of our model, in other words, this split doesn't fairly evaluate our model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building on holdout: Repeated Sub-sampling\n",
    "\n",
    "What we can do to get an evaluation metric with holdout with considerabily less evaluation variance is run holdout multiple times (keeping the split ratio constant) and averaging the accuracy metric for all runs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict as dd\n",
    "import random\n",
    "random.seed(3000)\n",
    "import numpy as np\n",
    "from NaiveBayes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(file, normal = True):\n",
    "    if(normal):\n",
    "        df = pd.read_csv(\"./2018S1-proj1_data/\"+file+\".csv\",header=None)\n",
    "        unnamed = df.columns[len(df.columns)-1]\n",
    "        df.rename(columns={unnamed:'class'},inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "car_dataset = preprocess(\"car\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb = NaiveBayes(car_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_test_split(df, split):\n",
    "    \n",
    "    train_percent = split[0]\n",
    "    test_percent = split[1]\n",
    "    \n",
    "    print(train_percent, test_percent)\n",
    "    \n",
    "    df_shuffled = df.sample(frac=1).reset_index(drop=True)\n",
    "        \n",
    "    train_portion_index = int(len(df_shuffled.index)*train_percent)\n",
    "    \n",
    "    test_portion_index = train_portion_index+1\n",
    "    \n",
    "    print (train_portion_index, test_portion_index)\n",
    "    \n",
    "    \n",
    "    train_portion = df_shuffled[:train_portion_index].reset_index(drop=True)\n",
    "    \n",
    "    test_portion = df_shuffled[test_portion_index-1:].reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    return train_portion, test_portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_holdout_90_10 = NaiveBayes(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "predict() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-013401c98147>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb_holdout_90_10\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: predict() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "results = nb_holdout_90_10.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
